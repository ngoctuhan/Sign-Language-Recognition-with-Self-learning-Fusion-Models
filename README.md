# Sign Language Recognition with Self-learning Fusion Models

## Overview

This repository contains trained models reported in the paper "[Sign Language Recognition with Self-learning Fusion Models]()"

In this paper, we introduced about a new structure for improve the performance of the model video classification on 3 datasets MHAD, UTD-MHAD & our dataset (VSL).

Main idea of this paper, using two network to can make a fusion result to support for improve accuracy of video classification on RGB image. 

In this paper, we also introduced a method self-learning to can create a new second model to can fusion with model video classification base on I3D in paper "[Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset](https://arxiv.org/abs/1705.07750)" by Joao Carreira and Andrew Zisserman. The paper was posted on arXiv in May 2017, and will be published as a
CVPR 2017 conference paper.

Beside that, we also provide a dataset for sign language include: 50 classes with sensor data & video data from 12 people.
## Running the code

### Requirements

